{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e9d054-1f03-4a2f-bcf1-eff11063c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ae37a1-dd1c-4792-b1c3-6558a352d0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0\n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0\n",
       "2                          Собаке - собачья смерть\\n    1.0\n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0\n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/labeled.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad651e5c-c956-484c-83aa-6a45f96108e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\n",
    "    \"и\", \"в\", \"на\", \"с\", \"для\", \"по\", \"к\", \"из\", \"о\", \"за\", \"от\", \"кто\", \"что\", \"это\", \"так\", \"как\", \"такой\", \"не\", \"да\", \"же\", \"может\", \"быть\", \"я\"\n",
    "])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-zа-яё\\s]', '', text)  \n",
    "    tokens = tokenizer.tokenize(text)  \n",
    "\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "data['processed_text'] = data['comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e3bc87-0940-41bf-bce5-6fecb7609820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(data['processed_text'], \n",
    "                 vector_size=64,       \n",
    "                 window=5,             \n",
    "                 min_count=5,          \n",
    "                 sg=0)                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec9b8c8-4487-4bd2-ab65-af25638730d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vector(tokens, model):\n",
    "    word_vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return None#np.zeros(model.vector_size) \n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "data['text_vector'] = data['processed_text'].apply(lambda x: get_text_vector(x, model))\n",
    "data = data.dropna(subset=['text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ab2884-1f19-4533-9a93-2287880323ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.92      0.82      2838\n",
      "         1.0       0.69      0.35      0.46      1446\n",
      "\n",
      "    accuracy                           0.73      4284\n",
      "   macro avg       0.71      0.63      0.64      4284\n",
      "weighted avg       0.72      0.73      0.70      4284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data['text_vector'].tolist()) \n",
    "y = data['toxic'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf1 = RandomForestClassifier(class_weight='balanced')\n",
    "clf2 = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "clf3 = SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rand', clf1), ('lr', clf2), ('SVC', clf3)], voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6278e085-641f-4e90-8851-dfe508c9c554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test = 'любовь'\n",
    "#'любовь это самое главное в нашей жизни'\n",
    "#'хватит лежать'\n",
    "#'это наш общий ребенок'\n",
    "\n",
    "tokens = preprocess_text(simple_test)  \n",
    "text_vector = get_text_vector(tokens, model) \n",
    "\n",
    "y_pred = clf1.predict([text_vector])\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c482044e-8596-4cda-80e0-191f2745d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = [model.wv[token] for token in data['processed_text'][0] if token in model.wv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc5a29-2cc2-4c37-bc2a-578fb17e8a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.92      0.89      2838\n",
      "         1.0       0.82      0.71      0.76      1446\n",
      "\n",
      "    accuracy                           0.85      4284\n",
      "   macro avg       0.84      0.82      0.83      4284\n",
      "weighted avg       0.85      0.85      0.85      4284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BOW\n",
    "data['processed_text_str'] = data['processed_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "y = data['toxic'] \n",
    "X = vectorizer.fit_transform(data['processed_text_str'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = SVC(kernel='linear', class_weight='balanced', probability=True)#RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8ff0f94-eb3f-4ded-830c-4230972fa1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test = 'это наш общий ребенок'#'хватит лежать'#'любовь это самое главное в нашей жизни'\n",
    "#'хватит лежать'\n",
    "#'это наш общий ребенок'\n",
    "processed_input = preprocess_text(simple_test)\n",
    "\n",
    "processed_input_str = ' '.join(processed_input)\n",
    "\n",
    "X_user = vectorizer.transform([processed_input_str])\n",
    "\n",
    "clf.predict(X_user)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6612b531-6bd4-44ec-9f6c-78fa77c0bfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_vector</th>\n",
       "      <th>processed_text_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[верблюдовто, дебилы, бл]</td>\n",
       "      <td>[0.20458055, -0.20117, 0.09967521, 0.06201748,...</td>\n",
       "      <td>верблюдовто дебилы бл</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[хохлы, отдушина, затюканого, россиянина, мол,...</td>\n",
       "      <td>[0.76204073, -0.75936, 0.5044583, 0.28179464, ...</td>\n",
       "      <td>хохлы отдушина затюканого россиянина мол вон а...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[собаке, собачья, смерть]</td>\n",
       "      <td>[0.1356669, -0.13263024, 0.06666818, 0.0511709...</td>\n",
       "      <td>собаке собачья смерть</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорблени...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[страницу, обнови, дебил, тоже, оскорбление, а...</td>\n",
       "      <td>[0.74406666, -0.76515055, 0.4315863, 0.2039924...</td>\n",
       "      <td>страницу обнови дебил тоже оскорбление а доказ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скр...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[тебя, убедил, страничный, пдф, том, скрипалей...</td>\n",
       "      <td>[0.61351424, -0.6146099, 0.382457, 0.12391656,...</td>\n",
       "      <td>тебя убедил страничный пдф том скрипалей отрав...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  toxic  \\\n",
       "0               Верблюдов-то за что? Дебилы, бл...\\n    1.0   \n",
       "1  Хохлы, это отдушина затюканого россиянина, мол...    1.0   \n",
       "2                          Собаке - собачья смерть\\n    1.0   \n",
       "3  Страницу обнови, дебил. Это тоже не оскорблени...    1.0   \n",
       "4  тебя не убедил 6-страничный пдф в том, что Скр...    1.0   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0                          [верблюдовто, дебилы, бл]   \n",
       "1  [хохлы, отдушина, затюканого, россиянина, мол,...   \n",
       "2                          [собаке, собачья, смерть]   \n",
       "3  [страницу, обнови, дебил, тоже, оскорбление, а...   \n",
       "4  [тебя, убедил, страничный, пдф, том, скрипалей...   \n",
       "\n",
       "                                         text_vector  \\\n",
       "0  [0.20458055, -0.20117, 0.09967521, 0.06201748,...   \n",
       "1  [0.76204073, -0.75936, 0.5044583, 0.28179464, ...   \n",
       "2  [0.1356669, -0.13263024, 0.06666818, 0.0511709...   \n",
       "3  [0.74406666, -0.76515055, 0.4315863, 0.2039924...   \n",
       "4  [0.61351424, -0.6146099, 0.382457, 0.12391656,...   \n",
       "\n",
       "                                  processed_text_str  \n",
       "0                              верблюдовто дебилы бл  \n",
       "1  хохлы отдушина затюканого россиянина мол вон а...  \n",
       "2                              собаке собачья смерть  \n",
       "3  страницу обнови дебил тоже оскорбление а доказ...  \n",
       "4  тебя убедил страничный пдф том скрипалей отрав...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a3ee44e-18a2-445c-829f-f6b95d0b4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.txt', 'w') as f:\n",
    "    f.write('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64cb4de-ddb0-44c6-8286-4b5236227c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test"
     ]
    }
   ],
   "source": [
    "!cat result.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
